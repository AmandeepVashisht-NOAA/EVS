Running prologue on parent mom node: nid001413...
Job 17242472.cbqs01 nodelist: nid001413
Job 17242472.cbqs01 - Prologue complete. Execution time: 3 seconds
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=00 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17242478.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=01 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17242508.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=02 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17242546.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=03 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17242708.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=04 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17242731.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=05 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17242782.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=06 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17242815.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=07 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17242846.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=08 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17242920.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=09 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17242981.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=10 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17242998.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=11 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17243032.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=12 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17243055.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=13 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17243182.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=14 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17243220.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=15 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17243246.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=16 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17243271.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=17 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17243301.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=18 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17243350.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=19 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17243478.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=20 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17243534.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=21 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17243546.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=22 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17243616.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=23 /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
17243785.cbqs01
++ sleep 60
++ exit
Job 17242472.cbqs01 - Epilogue complete. Execution time: 2 seconds
==================================================================
BEGIN - DEBUG INFO
==================================================================
Job 17242472.cbqs01 - Post Job Mem Usage:
nid001413: 2022-10-31.15 1 ===========================================
nid001413: 2022-10-31.15 2 #### Node Memory Usage for nid001413 ####
nid001413: 2022-10-31.15 3 ------------------
nid001413: 2022-10-31.15 4 Mem: total:527077116 used:10926144 free:509406048 shared:4787252 cache:6744924 avail:509002576
nid001413: 2022-10-31.15 5 4.0K	/dev/shm
nid001413: 2022-10-31.15 6 17M	/tmp
nid001413: 2022-10-31.15 7 ===========================================
nid001413: 2022-10-31.26 1 ===========================================
nid001413: 2022-10-31.26 2 #### Node Memory Usage for nid001413 ####
nid001413: 2022-10-31.26 3 ------------------
nid001413: 2022-10-31.26 4 Mem: total:527077116 used:10936724 free:509477320 shared:4784824 cache:6663072 avail:509032784
nid001413: 2022-10-31.26 5 4.0K	/dev/shm
nid001413: 2022-10-31.26 6 17M	/tmp
nid001413: 2022-10-31.26 7 ===========================================

------------------------------------------------------------------
Job 17242472.cbqs01 - /var/log/messages for job duration:
nid001413: 2022-10-31A--:--:--.------+--:-- nid001413 #### nid001413 - Job 17242472.cbqs01 Runtime Data from /var/log/messages
nid001413: 2022-10-31T14:07:24.583617+00:00 nid001413 prologue: MARK: Job 17242472.cbqs01 Start
nid001413: 2022-10-31T14:07:24.588770+00:00 nid001413 prologue: Job 17242472.cbqs01 nodelist: nid001413
nid001413: 2022-10-31T14:07:24.590525+00:00 nid001413 prologue: Job 17242472.cbqs01 - Checking existing ASLR settings...
nid001413: 2022-10-31T14:07:24.595733+00:00 nid001413 prologue: Job 17242472.cbqs01 - Checking palsd open file count...
nid001413: 2022-10-31T14:07:24.709018+00:00 nid001413 PBS_CMD: root : /opt/pbs/bin/qstat -Bf
nid001413: 2022-10-31T14:07:26.153923+00:00 nid001413 prologue: Job 17242472.cbqs01 - Checking one-shot control: False
nid001413: 2022-10-31T14:07:26.154937+00:00 nid001413 prologue: Job 17242472.cbqs01 - Recording pre-job HSN counters...
nid001413: 2022-10-31T14:07:26.270144+00:00 nid001413 prologue: Job 17242472.cbqs01 - Recording pre-job memory usage...
nid001413: 2022-10-31T14:07:26.358129+00:00 nid001413 prologue: Job 17242472.cbqs01 - Killing any stray user processes...
nid001413: 2022-10-31T14:07:26.430586+00:00 nid001413 prologue: Job 17242472.cbqs01 - Enabling turboboost...
nid001413: 2022-10-31T14:07:26.433290+00:00 nid001413 prologue: Job 17242472.cbqs01 - Verifying post-boot workarounds...
nid001413: 2022-10-31T14:07:26.780234+00:00 nid001413 prologue: Job 17242472.cbqs01 - Warchk Complete
nid001413: 2022-10-31T14:07:26.781335+00:00 nid001413 prologue: Job 17242472.cbqs01 - Recording initial NFS client statistics...
nid001413: 2022-10-31T14:07:26.787951+00:00 nid001413 prologue: Job 17242472.cbqs01 - Prologue complete. Execution time: 3 seconds
nid001413: 2022-10-31T14:07:36.712233+00:00 nid001413 systemd[1]: session-377889.scope: Succeeded.
nid001413: 2022-10-31T14:07:44.921820+00:00 nid001413 systemd[1]: session-377890.scope: Succeeded.
nid001413: 2022-10-31T14:08:16.523892+00:00 nid001413 systemd[1]: session-377891.scope: Succeeded.
nid001413: 2022-10-31T14:08:24.844983+00:00 nid001413 systemd[1]: session-377892.scope: Succeeded.
nid001413: 2022-10-31T14:08:57.029382+00:00 nid001413 systemd[1]: session-377893.scope: Succeeded.
nid001413: 2022-10-31T14:09:05.827020+00:00 nid001413 systemd[1]: session-377894.scope: Succeeded.
nid001413: 2022-10-31T14:09:36.417757+00:00 nid001413 systemd[1]: session-377895.scope: Succeeded.
nid001413: 2022-10-31T14:09:44.809309+00:00 nid001413 systemd[1]: session-377896.scope: Succeeded.
nid001413: 2022-10-31T14:10:24.227364+00:00 nid001413 systemd[1]: session-377897.scope: Succeeded.
nid001413: 2022-10-31T14:10:32.571265+00:00 nid001413 systemd[1]: session-377898.scope: Succeeded.
nid001413: 2022-10-31T14:10:56.471459+00:00 nid001413 systemd[1]: session-377899.scope: Succeeded.
nid001413: 2022-10-31T14:11:04.859953+00:00 nid001413 systemd[1]: session-377900.scope: Succeeded.
nid001413: 2022-10-31T14:11:36.500003+00:00 nid001413 systemd[1]: session-377901.scope: Succeeded.
nid001413: 2022-10-31T14:11:44.842523+00:00 nid001413 systemd[1]: session-377902.scope: Succeeded.
nid001413: 2022-10-31T14:12:16.489232+00:00 nid001413 systemd[1]: session-377903.scope: Succeeded.
nid001413: 2022-10-31T14:12:24.878258+00:00 nid001413 systemd[1]: session-377904.scope: Succeeded.
nid001413: 2022-10-31T14:12:56.472895+00:00 nid001413 systemd[1]: session-377905.scope: Succeeded.
nid001413: 2022-10-31T14:13:04.860724+00:00 nid001413 systemd[1]: session-377906.scope: Succeeded.
nid001413: 2022-10-31T14:13:36.551218+00:00 nid001413 systemd[1]: session-377907.scope: Succeeded.
nid001413: 2022-10-31T14:13:44.924957+00:00 nid001413 systemd[1]: session-377908.scope: Succeeded.
nid001413: 2022-10-31T14:14:16.481719+00:00 nid001413 systemd[1]: session-377909.scope: Succeeded.
nid001413: 2022-10-31T14:14:24.878843+00:00 nid001413 systemd[1]: session-377910.scope: Succeeded.
nid001413: 2022-10-31T14:14:38.098707+00:00 nid001413 systemd[1]: etc_update.service: Succeeded.
nid001413: 2022-10-31T14:14:56.600982+00:00 nid001413 systemd[1]: session-377911.scope: Succeeded.
nid001413: 2022-10-31T14:15:04.991278+00:00 nid001413 systemd[1]: session-377912.scope: Succeeded.
nid001413: 2022-10-31T14:15:44.099105+00:00 nid001413 systemd[1]: session-377913.scope: Succeeded.
nid001413: 2022-10-31T14:15:55.790851+00:00 nid001413 systemd[1]: session-377914.scope: Succeeded.
nid001413: 2022-10-31T14:16:21.569135+00:00 nid001413 systemd[1]: session-377915.scope: Succeeded.
nid001413: 2022-10-31T14:16:29.985844+00:00 nid001413 systemd[1]: session-377916.scope: Succeeded.
nid001413: 2022-10-31T14:17:01.566463+00:00 nid001413 systemd[1]: session-377917.scope: Succeeded.
nid001413: 2022-10-31T14:17:09.977104+00:00 nid001413 systemd[1]: session-377918.scope: Succeeded.
nid001413: 2022-10-31T14:17:41.831326+00:00 nid001413 systemd[1]: session-377919.scope: Succeeded.
nid001413: 2022-10-31T14:17:50.284176+00:00 nid001413 systemd[1]: session-377920.scope: Succeeded.
nid001413: 2022-10-31T14:18:21.656935+00:00 nid001413 systemd[1]: session-377921.scope: Succeeded.
nid001413: 2022-10-31T14:18:30.067054+00:00 nid001413 systemd[1]: session-377922.scope: Succeeded.
nid001413: 2022-10-31T14:18:56.484090+00:00 nid001413 systemd[1]: session-377923.scope: Succeeded.
nid001413: 2022-10-31T14:19:04.885444+00:00 nid001413 systemd[1]: session-377924.scope: Succeeded.
nid001413: 2022-10-31T14:19:36.483214+00:00 nid001413 systemd[1]: session-377925.scope: Succeeded.
nid001413: 2022-10-31T14:19:45.174627+00:00 nid001413 systemd[1]: session-377926.scope: Succeeded.
nid001413: 2022-10-31T14:20:27.774796+00:00 nid001413 systemd[1]: session-377927.scope: Succeeded.
nid001413: 2022-10-31T14:20:36.171114+00:00 nid001413 systemd[1]: session-377928.scope: Succeeded.
nid001413: 2022-10-31T14:21:06.633294+00:00 nid001413 systemd[1]: session-377929.scope: Succeeded.
nid001413: 2022-10-31T14:21:15.155922+00:00 nid001413 systemd[1]: session-377930.scope: Succeeded.
nid001413: 2022-10-31T14:21:46.517863+00:00 nid001413 systemd[1]: session-377931.scope: Succeeded.
nid001413: 2022-10-31T14:21:54.905130+00:00 nid001413 systemd[1]: session-377932.scope: Succeeded.
nid001413: 2022-10-31T14:22:26.475241+00:00 nid001413 systemd[1]: session-377933.scope: Succeeded.
nid001413: 2022-10-31T14:22:34.886860+00:00 nid001413 systemd[1]: session-377934.scope: Succeeded.
nid001413: 2022-10-31T14:23:06.562229+00:00 nid001413 systemd[1]: session-377935.scope: Succeeded.
nid001413: 2022-10-31T14:23:14.967625+00:00 nid001413 systemd[1]: session-377936.scope: Succeeded.
nid001413: 2022-10-31T14:23:46.545968+00:00 nid001413 systemd[1]: session-377937.scope: Succeeded.
nid001413: 2022-10-31T14:23:54.935938+00:00 nid001413 systemd[1]: session-377938.scope: Succeeded.
nid001413: 2022-10-31T14:24:00.090541+00:00 nid001413 systemd[1]: etc_update.service: Succeeded.
nid001413: 2022-10-31T14:24:26.465198+00:00 nid001413 systemd[1]: session-377939.scope: Succeeded.
nid001413: 2022-10-31T14:24:34.873558+00:00 nid001413 systemd[1]: session-377940.scope: Succeeded.
nid001413: 2022-10-31T14:25:06.511471+00:00 nid001413 systemd[1]: session-377941.scope: Succeeded.
nid001413: 2022-10-31T14:25:14.890312+00:00 nid001413 systemd[1]: session-377942.scope: Succeeded.
nid001413: 2022-10-31T14:25:46.506377+00:00 nid001413 systemd[1]: session-377943.scope: Succeeded.
nid001413: 2022-10-31T14:25:54.907642+00:00 nid001413 systemd[1]: session-377944.scope: Succeeded.
nid001413: 2022-10-31T14:26:26.521910+00:00 nid001413 systemd[1]: session-377945.scope: Succeeded.
nid001413: 2022-10-31T14:26:34.932590+00:00 nid001413 systemd[1]: session-377946.scope: Succeeded.
nid001413: 2022-10-31T14:27:06.567143+00:00 nid001413 systemd[1]: session-377947.scope: Succeeded.
nid001413: 2022-10-31T14:27:14.913958+00:00 nid001413 systemd[1]: session-377948.scope: Succeeded.
nid001413: 2022-10-31T14:27:46.577345+00:00 nid001413 systemd[1]: session-377949.scope: Succeeded.
nid001413: 2022-10-31T14:27:54.962415+00:00 nid001413 systemd[1]: session-377950.scope: Succeeded.
nid001413: 2022-10-31T14:28:26.480904+00:00 nid001413 systemd[1]: session-377951.scope: Succeeded.
nid001413: 2022-10-31T14:28:34.881743+00:00 nid001413 systemd[1]: session-377952.scope: Succeeded.
nid001413: 2022-10-31T14:29:06.530427+00:00 nid001413 systemd[1]: session-377953.scope: Succeeded.
nid001413: 2022-10-31T14:29:14.895506+00:00 nid001413 systemd[1]: session-377954.scope: Succeeded.
nid001413: 2022-10-31T14:29:47.107733+00:00 nid001413 systemd[1]: session-377955.scope: Succeeded.
nid001413: 2022-10-31T14:29:55.451103+00:00 nid001413 systemd[1]: session-377956.scope: Succeeded.
nid001413: 2022-10-31T14:30:41.254258+00:00 nid001413 systemd[1]: session-377957.scope: Succeeded.
nid001413: 2022-10-31T14:30:49.680203+00:00 nid001413 systemd[1]: session-377958.scope: Succeeded.
nid001413: 2022-10-31T14:31:21.524631+00:00 nid001413 systemd[1]: session-377959.scope: Succeeded.
nid001413: 2022-10-31T14:31:29.925046+00:00 nid001413 systemd[1]: session-377960.scope: Succeeded.
nid001413: 2022-10-31T14:32:01.658364+00:00 nid001413 systemd[1]: session-377961.scope: Succeeded.
nid001413: 2022-10-31T14:32:10.107802+00:00 nid001413 systemd[1]: session-377962.scope: Succeeded.
nid001413: 2022-10-31T14:32:15.685508+00:00 nid001413 epilogue: Job 17242472.cbqs01 complete, running post-job actions.
nid001413: 2022-10-31T14:32:15.702847+00:00 nid001413 epilogue: Job 17242472.cbqs01 - Recording post-job HSN counters...
nid001413: 2022-10-31T14:32:15.811128+00:00 nid001413 epilogue: Job 17242472.cbqs01 - Recording post-job memory usage...
nid001413: 2022-10-31T14:32:15.829545+00:00 nid001413 epilogue: Job 17242472.cbqs01 - Recording job NFS statistics to /tmp/nfsstats.17242472.cbqs01
nid001413: 2022-10-31T14:32:16.328138+00:00 nid001413 epilogue: Job 17242472.cbqs01 - Clearing /tmp...
nid001413: 2022-10-31T14:32:16.587665+00:00 nid001413 epilogue: Job 17242472.cbqs01 - Clearing shared memory...
nid001413: 2022-10-31T14:32:16.601151+00:00 nid001413 epilogue: Job 17242472.cbqs01 - Clearing memory cache...
nid001413: 2022-10-31T14:32:16.980589+00:00 nid001413 kernel: [7607855.511834] drop_caches (233657): drop_caches: 3
nid001413: 2022-10-31T14:32:17.135761+00:00 nid001413 epilogue: Job 17242472.cbqs01 - Releasing Lustre Locks...
nid001413: 2022-10-31T14:32:17.149706+00:00 nid001413 epilogue: MARK: Job 17242472.cbqs01 Complete.

------------------------------------------------------------------
Job 17242472.cbqs01 - dmesg output for job duration:
nid001413: 2022-10-31A--:--:--.------+--:-- nid001413 #### nid001413 - Job 17242472.cbqs01 Runtime Data from dmesg
nid001413: [Mon Oct 31 14:04:47 2022] MARK: Job 17242472.cbqs01 Start
nid001413: [Mon Oct 31 14:29:39 2022] MARK: Job 17242472.cbqs01 Complete.
nid001413: [Mon Oct 31 14:29:39 2022] drop_caches (233657): drop_caches: 3

------------------------------------------------------------------
Job 17242472.cbqs01 - Pre/Post job diff on HSN (MLX) Counters:
nid001413: 2022-10-31A--:--:--.------+--:-- nid001413 #### nid001413 - Job 17242472.cbqs01 HSN0 MLX Counter Post-Job Difference
nid001413: multicast: 300173124					      |	multicast: 300173263
nid001413: port_rcv_data: 593566762974677				      |	port_rcv_data: 593566763300909
nid001413: port_rcv_packets: 899700456428				      |	port_rcv_packets: 899700462984
nid001413: port_xmit_data: 602333547755495				      |	port_xmit_data: 602333548099255
nid001413: port_xmit_packets: 906089882570				      |	port_xmit_packets: 906089889126
nid001413: rx_bytes: 642807070968					      |	rx_bytes: 642816216807
nid001413: rx_packets: 751285739					      |	rx_packets: 751299944
nid001413: tx_bytes: 283725838338					      |	tx_bytes: 283728239591
nid001413: tx_packets: 323990424					      |	tx_packets: 324005631
nid001413: unicast_rcv_packets: 899700456428			      |	unicast_rcv_packets: 899700462984
nid001413: unicast_xmit_packets: 906089882570			      |	unicast_xmit_packets: 906089889126

------------------------------------------------------------------
### Job 17242472.cbqs01 - NFS Statistics for job duration:
nid001413: #### 2022-10-31.00 #### nid001413 - Job 17242472.cbqs01 NFS Statistics for job duration (nfsstat)
nid001413: ## /usr/sbin/nfsstat -v -S /tmp/nfsstats.begin.17242472.cbqs01 :
nid001413: Client packet stats:
nid001413: packets    udp        tcp        tcpconn
nid001413: 0          0          0          0       
nid001413: 
nid001413: Client rpc stats:
nid001413: calls      retrans    authrefrsh
nid001413: 9393       0          9393    
nid001413: 
nid001413: Client nfs v3:
nid001413: null             getattr          setattr          lookup           access           
nid001413: 0         0%     8084     86%     2         0%     349       3%     382       4%     
nid001413: readlink         read             write            create           mkdir            
nid001413: 48        0%     286       3%     47        0%     0         0%     0         0%     
nid001413: symlink          mknod            remove           rmdir            rename           
nid001413: 0         0%     0         0%     0         0%     0         0%     0         0%     
nid001413: link             readdir          readdirplus      fsstat           fsinfo           
nid001413: 0         0%     0         0%     195       2%     0         0%     0         0%     
nid001413: pathconf         commit           
nid001413: 0         0%     0         0%     
nid001413: 
nid001413: 
nid001413: -----------------------------------------------------------------------------------
nid001413: 
nid001413: #### 2022-10-31.00 #### nid001413 - Job 17242472.cbqs01 NFS Mount Statistics for job duration (mountstats)
nid001413: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.17242472.cbqs01 /apps :
nid001413: 
nid001413: 
nid001413: 172.20.250.16:/AZ-HFS-Cactus-apps mounted on /apps:
nid001413: 
nid001413:            ops/s       rpc bklog
nid001413:            5.423           0.000
nid001413: 
nid001413: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001413:                    0.053           0.112           2.119        0 (0.0%)           0.241           0.304
nid001413: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001413:                    0.000           0.000           0.000        0 (0.0%)           0.000           0.000
nid001413: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.17242472.cbqs01 /sfs :
nid001413: 
nid001413: 
nid001413: 172.20.250.16:/AZ-HFS-Cactus-sfs mounted on /sfs:
nid001413: 
nid001413:            ops/s       rpc bklog
nid001413:            5.423           0.000
nid001413: 
nid001413: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001413:                    0.001           0.000           0.391        0 (0.0%)           0.000           0.000
nid001413: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001413:                    0.032           0.119           3.766        0 (0.0%)           0.553           0.660
nid001413: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.17242472.cbqs01 /u :
nid001413: 
nid001413: 
nid001413: 172.20.250.17:/AZ-HFS-Cactus-u mounted on /u:
nid001413: 
nid001413:            ops/s       rpc bklog
nid001413:            0.060           0.000
nid001413: 
nid001413: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001413:                    0.001           0.001           0.969        0 (0.0%)           0.000           0.000
nid001413: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001413:                    0.000           0.000           0.000        0 (0.0%)           0.000           0.000
nid001413: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.17242472.cbqs01 /pe :
nid001413: 
nid001413: 
nid001413: 10.31.62.244:/cm_shared/image/images_rw_nfs/pe mounted on /pe:
nid001413: 
nid001413:            ops/s       rpc bklog
nid001413:            0.894           0.000
nid001413: 
nid001413: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001413:                    0.024           4.023         166.402        0 (0.0%)           3.000           3.083
nid001413: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001413:                    0.000           0.000           0.000        0 (0.0%)           0.000           0.000
nid001413: ----------------
nid001413: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.17242472.cbqs01 /apps :
nid001413: Stats for 172.20.250.16:/AZ-HFS-Cactus-apps mounted on /apps:
nid001413:   NFS mount options: ro,vers=3,rsize=65536,wsize=65536,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=172.20.250.16,mountvers=3,mountport=4048,mountproto=udp,local_lock=none
nid001413:   NFS server capabilities: caps=0x3fc7,wtmult=4096,dtsize=8192,bsize=0,namlen=255
nid001413:   NFS security flavor: 1  pseudoflavor: 0
nid001413: 
nid001413: NFS byte counts:
nid001413:   applications read 5873356 bytes via read(2)
nid001413:   applications wrote 0 bytes via write(2)
nid001413:   applications read 0 bytes via O_DIRECT read(2)
nid001413:   applications wrote 0 bytes via O_DIRECT write(2)
nid001413:   client read 148537 bytes via NFS READ
nid001413:   client wrote 0 bytes via NFS WRITE
nid001413: 
nid001413: RPC statistics:
nid001413:   8075 RPC requests sent, 8075 RPC replies received (0 XIDs not found)
nid001413:   average backlog queue length: 0
nid001413: 
nid001413: GETATTR:
nid001413: 	7126 ops (88%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 112
nid001413: 	backlog wait: 0.003087 	RTT: 0.193236 	total execute time: 0.210918 (milliseconds)
nid001413: ACCESS:
nid001413: 	313 ops (3%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 120
nid001413: 	backlog wait: 0.003195 	RTT: 0.546326 	total execute time: 0.562300 (milliseconds)
nid001413: READDIRPLUS:
nid001413: 	195 ops (2%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 931
nid001413: 	backlog wait: 0.000000 	RTT: 0.251282 	total execute time: 0.266667 (milliseconds)
nid001413: LOOKUP:
nid001413: 	173 ops (2%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 164
nid001413: 	backlog wait: 0.005780 	RTT: 0.150289 	total execute time: 0.167630 (milliseconds)
nid001413: READ:
nid001413: 	79 ops (0%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 2009
nid001413: 	backlog wait: 0.037975 	RTT: 0.240506 	total execute time: 0.303797 (milliseconds)
nid001413: READLINK:
nid001413: 	46 ops (0%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 135
nid001413: 	backlog wait: 0.000000 	RTT: 0.130435 	total execute time: 0.152174 (milliseconds)
nid001413: 
nid001413: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.17242472.cbqs01 /sfs :
nid001413: 
nid001413: Stats for 172.20.250.16:/AZ-HFS-Cactus-sfs mounted on /sfs:
nid001413:   NFS mount options: rw,vers=3,rsize=65536,wsize=65536,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=172.20.250.16,mountvers=3,mountport=4048,mountproto=udp,local_lock=none
nid001413:   NFS server capabilities: caps=0x3fc7,wtmult=4096,dtsize=8192,bsize=0,namlen=255
nid001413:   NFS security flavor: 1  pseudoflavor: 0
nid001413: 
nid001413: NFS byte counts:
nid001413:   applications read 13847 bytes via read(2)
nid001413:   applications wrote 83651 bytes via write(2)
nid001413:   applications read 0 bytes via O_DIRECT read(2)
nid001413:   applications wrote 0 bytes via O_DIRECT write(2)
nid001413:   client read 146 bytes via NFS READ
nid001413:   client wrote 167307 bytes via NFS WRITE
nid001413: 
nid001413: RPC statistics:
nid001413:   8075 RPC requests sent, 8075 RPC replies received (0 XIDs not found)
nid001413:   average backlog queue length: 0
nid001413: 
nid001413: GETATTR:
nid001413: 	82 ops (1%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 112
nid001413: 	backlog wait: 0.012195 	RTT: 0.146341 	total execute time: 0.182927 (milliseconds)
nid001413: WRITE:
nid001413: 	47 ops (0%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 160
nid001413: 	backlog wait: 0.042553 	RTT: 0.553191 	total execute time: 0.659574 (milliseconds)
nid001413: ACCESS:
nid001413: 	6 ops (0%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 120
nid001413: 	backlog wait: 0.000000 	RTT: 0.166667 	total execute time: 0.333333 (milliseconds)
nid001413: LOOKUP:
nid001413: 	5 ops (0%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 240
nid001413: 	backlog wait: 0.000000 	RTT: 0.000000 	total execute time: 0.200000 (milliseconds)
nid001413: SETATTR:
nid001413: 	2 ops (0%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 144
nid001413: 	backlog wait: 0.000000 	RTT: 0.500000 	total execute time: 0.500000 (milliseconds)
nid001413: READ:
nid001413: 	1 ops (0%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 276
nid001413: 	backlog wait: 0.000000 	RTT: 0.000000 	total execute time: 0.000000 (milliseconds)
nid001413: 
nid001413: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.17242472.cbqs01 /u :
nid001413: 
nid001413: Stats for 172.20.250.17:/AZ-HFS-Cactus-u mounted on /u:
nid001413:   NFS mount options: rw,vers=3,rsize=65536,wsize=65536,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=172.20.250.17,mountvers=3,mountport=4048,mountproto=udp,local_lock=none
nid001413:   NFS server capabilities: caps=0x3fc7,wtmult=4096,dtsize=8192,bsize=0,namlen=255
nid001413:   NFS security flavor: 1  pseudoflavor: 0
nid001413: 
nid001413: NFS byte counts:
nid001413:   applications read 701 bytes via read(2)
nid001413:   applications wrote 0 bytes via write(2)
nid001413:   applications read 0 bytes via O_DIRECT read(2)
nid001413:   applications wrote 0 bytes via O_DIRECT write(2)
nid001413:   client read 701 bytes via NFS READ
nid001413:   client wrote 0 bytes via NFS WRITE
nid001413: 
nid001413: RPC statistics:
nid001413:   90 RPC requests sent, 90 RPC replies received (0 XIDs not found)
nid001413:   average backlog queue length: 0
nid001413: 
nid001413: GETATTR:
nid001413: 	48 ops (53%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 112
nid001413: 	backlog wait: 0.000000 	RTT: 0.208333 	total execute time: 0.208333 (milliseconds)
nid001413: ACCESS:
nid001413: 	29 ops (32%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 120
nid001413: 	backlog wait: 0.000000 	RTT: 0.137931 	total execute time: 0.137931 (milliseconds)
nid001413: LOOKUP:
nid001413: 	12 ops (13%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 136
nid001413: 	backlog wait: 0.000000 	RTT: 0.166667 	total execute time: 0.166667 (milliseconds)
nid001413: READ:
nid001413: 	1 ops (1%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 832
nid001413: 	backlog wait: 0.000000 	RTT: 0.000000 	total execute time: 0.000000 (milliseconds)
nid001413: 
nid001413: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.17242472.cbqs01 /pe :
nid001413: 
nid001413: Stats for 10.31.62.244:/cm_shared/image/images_rw_nfs/pe mounted on /pe:
nid001413:   NFS mount options: ro,vers=3,rsize=1048576,wsize=1048576,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,nolock,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.31.62.244,mountvers=3,mountport=38465,mountproto=tcp,local_lock=all
nid001413:   NFS server capabilities: caps=0x3fcf,wtmult=4096,dtsize=32768,bsize=0,namlen=255
nid001413:   NFS security flavor: 1  pseudoflavor: 0
nid001413: 
nid001413: NFS byte counts:
nid001413:   applications read 1261568 bytes via read(2)
nid001413:   applications wrote 0 bytes via write(2)
nid001413:   applications read 0 bytes via O_DIRECT read(2)
nid001413:   applications wrote 0 bytes via O_DIRECT write(2)
nid001413:   client read 6127616 bytes via NFS READ
nid001413:   client wrote 0 bytes via NFS WRITE
nid001413: 
nid001413: RPC statistics:
nid001413:   1331 RPC requests sent, 1331 RPC replies received (0 XIDs not found)
nid001413:   average backlog queue length: 0
nid001413: 
nid001413: GETATTR:
nid001413: 	828 ops (62%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 112
nid001413: 	backlog wait: 0.002415 	RTT: 0.353865 	total execute time: 0.371981 (milliseconds)
nid001413: LOOKUP:
nid001413: 	159 ops (11%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 82
nid001413: 	backlog wait: 0.000000 	RTT: 0.559748 	total execute time: 0.578616 (milliseconds)
nid001413: READ:
nid001413: 	36 ops (2%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 170255
nid001413: 	backlog wait: 0.055556 	RTT: 3.000000 	total execute time: 3.083333 (milliseconds)
nid001413: ACCESS:
nid001413: 	34 ops (2%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 36
nid001413: 	backlog wait: 0.000000 	RTT: 0.352941 	total execute time: 0.382353 (milliseconds)
nid001413: READLINK:
nid001413: 	2 ops (0%) 
nid001413: 	avg bytes sent per op: avg bytes received per op: 136
nid001413: 	backlog wait: 0.000000 	RTT: 0.000000 	total execute time: 0.500000 (milliseconds)
nid001413: 
nid001413: 

------------------------------------------------------------------
### Job 17242472.cbqs01 - Exit status is 0

------------------------------------------------------------------
Job 17242472.cbqs01 - Job summary:
Job Id: 17242472.cbqs01
    Job_Name = run_rtma_ru_stats
    Job_Owner = perry.shafran@clogin05.cactus.wcoss2.ncep.noaa.gov
    resources_used.cpupercent = 5
    resources_used.cput = 00:00:02
    resources_used.mem = 24240kb
    resources_used.ncpus = 1
    resources_used.vmem = 46256kb
    resources_used.walltime = 00:24:12
    job_state = R
    queue = dev
    server = cbqs01
    Account_Name = VERF-DEV
    Checkpoint = u
    ctime = Mon Oct 31 14:07:22 2022
    Error_Path = clogin05.cactus.wcoss2.ncep.noaa.gov:/lfs/h2/emc/vpppg/save/pe
	rry.shafran/EVS/ecf/realtime_analyses/stats/run_rtma_ru_stats.e17242472
	
    exec_host = nid001413/0
    exec_vnode = (nid001413:ncpus=1:mem=2097152kb)
    Hold_Types = n
    Join_Path = oe
    Keep_Files = oed
    Mail_Points = a
    mtime = Mon Oct 31 14:31:38 2022
    Output_Path = clogin05.cactus.wcoss2.ncep.noaa.gov:/lfs/h2/emc/vpppg/save/p
	erry.shafran/EVS/ecf/realtime_analyses/stats/run_rtma_ru_stats.o1724247
	2
    Priority = 0
    qtime = Mon Oct 31 14:07:22 2022
    Rerunable = False
    Resource_List.alvl = 2
    Resource_List.aslr = True
    Resource_List.debug = True
    Resource_List.dfs = False
    Resource_List.hyper = False
    Resource_List.mem = 2gb
    Resource_List.ncpus = 1
    Resource_List.nodect = 1
    Resource_List.one-shot = False
    Resource_List.place = shared
    Resource_List.select = 1:ncpus=1:mem=2GB
    Resource_List.thp = True
    Resource_List.turbo = True
    Resource_List.walltime = 02:00:00
    schedselect = 1:ncpus=1:mem=2GB:prepost=False
    stime = Mon Oct 31 14:07:23 2022
    session_id = 226682
    Shell_Path_List = /bin/bash
    jobdir = /u/perry.shafran
    substate = 42
    Variable_List = PBS_O_HOME=/u/perry.shafran,PBS_O_LANG=en_US.UTF-8,
	PBS_O_LOGNAME=perry.shafran,
	PBS_O_PATH=/apps/ops/prod/nco/core/prod_util.v2.0.14/ush:/apps/prod/hp
	c-stack/intel-19.1.3.304/netcdf/4.7.4/bin:/apps/prod/hpc-stack/intel-19
	.1.3.304/hdf5/1.10.6/bin:/apps/spack/python/3.8.6/intel/19.1.3.304/pjn2
	nzkjvqgmjw4hmyz43v5x4jbxjzpk/bin:/apps/ops/para/libs/intel/19.1.3.304/m
	etplus/4.1.1/ush:/apps/ops/para/libs/intel/19.1.3.304/met/10.1.1/bin:/a
	pps/ops/para/libs/intel/19.1.3.304/gsl/2.6/bin:/pe/intel/compilers_and_
	libraries_2020.4.304/linux/bin/intel64:/pe/intel/compilers_and_librarie
	s_2020.4.304/linux/bin:/pe/intel/compilers_and_libraries_2020.4.304/lin
	ux/mpi/intel64/bin:/pe/intel/debugger_2020/gdb/intel64/bin:/opt/cray/li
	bfabric/1.11.0.0./bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt
	/sgi/bin:/usr/local/bin:/usr/bin:/bin:/usr/lib/mit/bin:/usr/lib/mit/sbi
	n:/opt/pbs/bin:/sbin:.:/u/perry.shafran/bin:/usr/sbin:/apps/prod/python
	-modules/3.8.6/intel/19.1.3.304/bin:/apps/prod/python-modules/3.8.6/int
	el/19.1.3.304/lib/python3.8/site-packages/bin,
	PBS_O_MAIL=/var/spool/mail/perry.shafran,PBS_O_SHELL=/bin/bash,
	PBS_O_WORKDIR=/lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_an
	alyses/stats,PBS_O_SYSTEM=Linux,PBS_O_QUEUE=dev,
	PBS_O_HOST=clogin05.cactus.wcoss2.ncep.noaa.gov
    euser = perry.shafran
    egroup = emc
    hashname = 17242472.cbqs01
    queue_rank = 1667225242177
    queue_type = E
    comment = Job run at Mon Oct 31 at 14:07 on (nid001413:ncpus=1:mem=2097152k
	b)
    etime = Mon Oct 31 14:07:22 2022
    umask = 22
    run_count = 6
    eligible_time = 00:00:00
    accrue_type = 3
    Submit_arguments = run_rtma_ru_stats.sh
    project = VERF-DEV
    run_version = 1
    Submit_Host = clogin05.cactus.wcoss2.ncep.noaa.gov


------------------------------------------------------------------
Job 17242472.cbqs01 - PBS tracejob output (for parent mom node only):

Job: 17242472.cbqs01

10/31/2022 14:07:23  M    update_job_usage: CPU usage: 0.000 secs
10/31/2022 14:07:23  M    update_job_usage: cpupercent initialized to zero
10/31/2022 14:07:23  M    update_job_usage: Memory usage: mem=0b
10/31/2022 14:07:23  M    no active tasks
10/31/2022 14:07:27  M    Started, pid = 226682
10/31/2022 14:07:52  M    update_job_usage: CPU usage: 1.747 secs
10/31/2022 14:07:52  M    update_job_usage: 17242472.cbqs01 measured interval cpupercent 5 increased job cpupercent to 5
10/31/2022 14:07:52  M    update_job_usage: Memory usage: mem=24240kb
10/31/2022 14:09:54  M    update_job_usage: CPU usage: 1.769 secs
10/31/2022 14:09:54  M    update_job_usage: Memory usage: mem=24240kb
10/31/2022 14:11:56  M    update_job_usage: CPU usage: 1.792 secs
10/31/2022 14:11:56  M    update_job_usage: Memory usage: mem=24240kb
10/31/2022 14:13:59  M    update_job_usage: CPU usage: 1.817 secs
10/31/2022 14:13:59  M    update_job_usage: Memory usage: mem=24240kb
10/31/2022 14:16:01  M    update_job_usage: CPU usage: 1.842 secs
10/31/2022 14:16:01  M    update_job_usage: Memory usage: mem=24240kb
10/31/2022 14:18:03  M    update_job_usage: CPU usage: 1.866 secs
10/31/2022 14:18:03  M    update_job_usage: Memory usage: mem=24240kb
10/31/2022 14:20:05  M    update_job_usage: CPU usage: 1.890 secs
10/31/2022 14:20:05  M    update_job_usage: Memory usage: mem=24240kb
10/31/2022 14:22:07  M    update_job_usage: CPU usage: 1.914 secs
10/31/2022 14:22:07  M    update_job_usage: Memory usage: mem=24240kb
10/31/2022 14:24:10  M    update_job_usage: CPU usage: 1.938 secs
10/31/2022 14:24:10  M    update_job_usage: Memory usage: mem=24240kb
10/31/2022 14:26:12  M    update_job_usage: CPU usage: 1.960 secs
10/31/2022 14:26:12  M    update_job_usage: Memory usage: mem=24240kb
10/31/2022 14:28:15  M    update_job_usage: CPU usage: 1.987 secs
10/31/2022 14:28:15  M    update_job_usage: Memory usage: mem=24240kb
10/31/2022 14:30:17  M    update_job_usage: CPU usage: 2.011 secs
10/31/2022 14:30:17  M    update_job_usage: Memory usage: mem=24240kb
10/31/2022 14:32:15  M    task 00000001 terminated
10/31/2022 14:32:15  M    Terminated
10/31/2022 14:32:15  M    task 00000001 cput=00:00:03
10/31/2022 14:32:15  M    kill_job
10/31/2022 14:32:15  M    nid001413 cput=00:00:02 mem=24240kb
10/31/2022 14:32:15  M    update_job_usage: CPU usage: 2.024 secs
10/31/2022 14:32:15  M    update_job_usage: Memory usage: mem=24240kb
10/31/2022 14:32:19  M    update_job_usage: No CPU usage data
10/31/2022 14:32:19  M    no active tasks

------------------------------------------------------------------
To see full PBS log data, run: /sfs/admin/scripts/tracejob.sh 17242472.cbqs01

==================================================================
END - DEBUG INFO
==================================================================

##### Job 17242472.cbqs01 - PBS Job Script:

#PBS -N run_rtma_ru_stats
#PBS -N run_rtma_ru_stats
#PBS -j oe
#PBS -j oe
#PBS -S /bin/bash
#PBS -S /bin/bash
#PBS -q "dev"
#PBS -q "dev"
#PBS -A VERF-DEV
#PBS -A VERF-DEV
#PBS -l walltime=02:00:00
#PBS -l walltime=02:00:00
#PBS -l select=1:ncpus=1:mem=2GB
#PBS -l select=1:ncpus=1:mem=2GB
#PBS -l debug=true
#PBS -l debug=true

set -x

for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
do
   export fhr
   qsub -v cyc=$fhr /lfs/h2/emc/vpppg/save/perry.shafran/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
   sleep 60
done

exit

##### End of job script
------------------------------------------------------------------
