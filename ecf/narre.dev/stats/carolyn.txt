
Binbin Zhou - NOAA Affiliate <binbin.zhou@noaa.gov>
Fri, Jun 10, 2:26 PM (4 days ago)
to HPC

WCOSS2 help desk,

  I have an issue for my job that contains both mpi and a serial process
 My running script file looks like below:

  Run program A 
  Run a MPI 
     mpiexec  -n 8 -ppn 8 --cpu-bind core --depth=2 cfp  run_8_processes.sh 
 
  The run_8_processes.sh file lists 8 processes script file names in sequence

In my job card, I set PBS as below 
 #PBS -l select=1:ncpus=16:mem=100GB

Submit the job,  program A can be run, but running MPI has error. 
The error message says:

CFP RANK   1    CFP TASK NUMBER: 0002 FAILED    
CFP RANK   2    CFP TASK NUMBER: 0003 FAILED
CFP RANK   3    CFP TASK NUMBER: 0004 FAILED
CFP RANK   4    CFP TASK NUMBER: 0005 FAILED
CFP RANK   5    CFP TASK NUMBER: 0006 FAILED
CFP RANK   6    CFP TASK NUMBER: 0007 FAILED
CFP RANK   7    CFP TASK NUMBER: 0008 FAILED

But if I remove the program A part (run it separately), the MPI can be run successfully. So I think it is  Node/CPU setting issue.

Note: This job can be run on WCOSS1 by setting #BSUB -n 8 
Since MPI processes depend on the output of program A,  I prefer to put it and MPI processes in the same script.

Could you please help me out of this issue?

Thanks!

Binbin 



--
Binbin Zhou 

IMSG at NOAA/NWS/NCEP/EMC

5830 University Research Ct. 

College Park, MD 20740

Binbin.Zhou@noaa.gov

301-683-3683


WCOSS2 Help
Jun 10, 2022, 2:43 PM (4 days ago)
to me

Dear Binbin Zhou - NOAA Affiliate,

Thank you for your request.

Can you please point me to your job card and stdoutstderr.  Also, is this on cactus or dogwood.
Thanks

Thanks


Your GDIT-WCOSS2 Team

Carolyn Pasti


Binbin Zhou - NOAA Affiliate <binbin.zhou@noaa.gov>
Jun 10, 2022, 2:59 PM (4 days ago)
to WCOSS2

On dogwood.
My job card is /lfs/h2/emc/vpppg/noscrub/binbin.zhou/EVS/evs.v1.0/ecf/narre.dev/stats/jevs_narre_stats.ecf
The output stdout and err files are in
/lfs/h2/emc/ptmp/binbin.zhou/evs/tmpnwprd/evs_narre_stat.o
/lfs/h2/emc/ptmp/binbin.zhou/evs/tmpnwprd/evs_narre_stat.e

This time I modified the PBS setting to be 
#PBS -l select=1:ncpus=2:mem=100GB+1:ncpus=16:mem=100GB

Then error message is gone, but MPI is not running. I guess it is due to MPI begins to run before
program A is finished? MPI should wait for program A to be finished. 

Binbin 




WCOSS2 Help
Jun 13, 2022, 2:59 PM (20 hours ago)
to me

Dear Binbin Zhou - NOAA Affiliate,

Thank you for your request.

Why are you setting OMP_THREADS=16, it should be =1  I just looked in your
/lfs/h2/emc/ptmp/binbin.zhou/evs/tmpnwprd/evs_narre_stat.e/lfs/h2/emc/ptmp/binbin.zhou/evs/tmpnwprd/evs_narre_stat.e    file and it should be set to 1

06/13 18:30:01.429 metplus (met_util.py:267) WARNING: Config variable OMP_NUM_THREADS (1) will be overridden by the environment variable OMP_NUM_THREADS (16)
06/13 18:30:01.430 metplus (master_metplus.py:50) WARNING: master_metplus.py ha
s been renamed to run_metplus.py. This script name will be removed in a future version.
forrtl: error (78): process kil;

Additionally, if you run have something like the following in your script
programA.exe
mpiexec  -n 8 -ppn 8 --cpu-bind core --depth=2 cfp  run_8_processes.sh

Then the mpiexec command will not run until programA is finished.   If you want it to run
at the same time then add it to the run_8_processes script and increase ncpus to 18 -n 8 to -n 9
and -ppn 9

Your GDIT-WCOSS2 Team

Carolyn Pasti


Binbin Zhou - NOAA Affiliate <binbin.zhou@noaa.gov>
Jun 13, 2022, 3:17 PM (20 hours ago)
to WCOSS2

Carolyn,

   I have removed program A and run it separately. 
I do not set $OMP_NUM_THREADS in my job card
So it is same as ncpus? 
 Do you mean the PBS setting should be like this
#PBS -l place=vscatter,select=1:ncpus=2:mem=100GB
and 
mpiexec  -n 8 -ppn 8 --cpu-bind core --depth=2 cfp  run_8_processes.sh 

Thanks!

Binbin


Binbin Zhou - NOAA Affiliate <binbin.zhou@noaa.gov>
Jun 13, 2022, 7:06 PM (16 hours ago)
to WCOSS2

Carolyn,

   Yes, after modified ncpus, the run is successful.
This ticket can be closed.

Thanks!

Binbin


WCOSS2 Help
10:49 AM (43 minutes ago)
to me

Dear Binbin Zhou - NOAA Affiliate,

Thank you for your request.
Hi Binbin,
I think you have a threaded code somewhere and it is defaulting to the number
of ncpus as threads.  We do not set OMP_NUM_THREADS=1 by default so I would
suggest you do that in your job card.

Additionally if you are running mpiexec -n 8 and -ppn 8 with depth of 2 you
should be asking for 16 ncpus, not 2 or it may start running very slow if they end up strictly
enforcing the CPU limit b/c you really are using 16 ncpus with 8 processes times 2 cores per process.



Your GDIT-WCOSS2 Team

Carolyn Pasti
